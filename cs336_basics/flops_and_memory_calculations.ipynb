{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider GPT-2 XL, which has the following configuration:\n",
    "-  vocab_size: 50,257\n",
    "-   context_length: 1,024\n",
    "-   num_layers: 48\n",
    "-   d_model: 1,600\n",
    "-   num_heads: 25\n",
    "-   d_ff: 6,400\n",
    "\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using singl-precision floating point, how much memory is required to just load this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params 1.64B\n",
      "Memory fp32: 6.10 GiB\n",
      "Memory bf16: 3.05 GiB\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "context_len = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400\n",
    "\n",
    "token_embeddings = vocab_size * d_model\n",
    "position_embeddings = context_len * d_model\n",
    "embd_layer = token_embeddings + position_embeddings\n",
    "\n",
    "q_k_v_o = 4 * d_model * d_model\n",
    "ffn = 2 * d_model * d_ff\n",
    "transformer_blocks = num_layers * (q_k_v_o + ffn)\n",
    "\n",
    "lm_head = d_model * vocab_size\n",
    "\n",
    "total_params = embd_layer + transformer_blocks + lm_head\n",
    "print(f\"Total trainable params {total_params/10**9:0.2f}B\")\n",
    "bytes_float32 = total_params * 4\n",
    "mem_fp32 = bytes_float32 / 1024 ** 3\n",
    "bytes_bf16 = total_params * 2\n",
    "mem_bf16 = bytes_bf16 / 1024 ** 3\n",
    "print(f\"Memory fp32: {mem_fp32:,.2f} GiB\")\n",
    "print(f\"Memory bf16: {mem_bf16:,.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model.\n",
    "How many FLOPs do these matrix multiplies require in total? Assume that our input\n",
    "sequence has `context_length` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_flops(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b\n",
    "):\n",
    "    F_forward_pass = get_forward_flops_TransformerLM(\n",
    "        vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "    )\n",
    "    F_adamw = get_adamw_flops(\n",
    "        vocab_size,\n",
    "        context_length,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        d_ff\n",
    "    )\n",
    "    F_backward_pass = get_backward_flops_TransformerLM(\n",
    "        F_forward_pass\n",
    "    )\n",
    "    return F_forward_pass + F_backward_pass + F_adamw\n",
    "\n",
    "def get_forward_flops_TransformerLM(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b\n",
    "):\n",
    "    F_transformer_blocks = get_flops_TransformerBlock(\n",
    "        b, context_length, d_model, num_heads, d_ff\n",
    "    ) * num_layers\n",
    "    F_ln_final = get_flops_RMSNorm(b, context_length, d_model)\n",
    "    F_lm_head = get_flops_lm_head(b, context_length, d_model, vocab_size)\n",
    "    return F_transformer_blocks + F_ln_final + F_lm_head\n",
    "\n",
    "def get_adamw_flops(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    d_ff\n",
    "):\n",
    "    # beta_1 * m + (1 - beta_1) * grad # 2 FLOPs\n",
    "    # beta_2 * v + (1 - beta_2) * (grad**2) # 3  FLOPs\n",
    "    # m / (1 - beta_1**t) # 1 FLOPs\n",
    "    # v / (1 - beta_2**t) # 1 FLOPs\n",
    "    #  (torch.sqrt(v) + eps) # 2 FLOPs\n",
    "    # m / (torch.sqrt(v) + eps) # 1 FLOPs\n",
    "    # p - lr * step - lr * wd * p # 4 FLOPs\n",
    "\n",
    "    # TOTAL CONSTANT FLOPs = 16 \n",
    "    total_params = (\n",
    "        2 * vocab_size * d_model  # token and lm head embeddings\n",
    "        + context_length * d_model  # positional embedding\n",
    "        + num_layers * (4 * d_model**2 + 2 * d_model * d_ff) # Transformer blocks: attention + FFN\n",
    "    )\n",
    "    return 16 * total_params\n",
    "\n",
    "def get_backward_flops_TransformerLM(\n",
    "    F_forward_pass\n",
    "):\n",
    "    F_backward_pass = 2 * F_forward_pass\n",
    "    return F_backward_pass\n",
    "\n",
    "\n",
    "def get_flops_lm_head(b, context_length, d_model, vocab_size):\n",
    "    return 2 * b * context_length * d_model * vocab_size\n",
    "\n",
    "def get_flops_TransformerBlock(b, context_length, d_model, num_heads, d_ff):\n",
    "    F_rmsnorm_1 = get_flops_RMSNorm(b, context_length, d_model)\n",
    "    F_mha = get_flops_MultiHeadAttention(b, context_length, d_model, num_heads)\n",
    "    F_rmsnorm_2 = F_rmsnorm_1\n",
    "    F_ffn = get_flops_FFN(b, context_length, d_model, d_ff)\n",
    "    return F_rmsnorm_1 + F_mha + F_rmsnorm_2 + F_ffn\n",
    "\n",
    "def get_flops_RMSNorm(b, context_length, d_model):\n",
    "    return (3*d_model + 2) * b * context_length\n",
    "\n",
    "def get_flops_MultiHeadAttention(b, context_length, d_model, num_heads):\n",
    "    F_QKV_linear_proj = get_qkv_proj(b, context_length, d_model)\n",
    "    F_attention = get_flops_atten_scores(b, context_length, d_model)\n",
    "    F_softmax = get_flops_Softmax(b, context_length, num_heads)\n",
    "    F_weighted = get_flops_atten_scores(b, context_length, d_model)\n",
    "    F_out_proj = get_flops_output_proj(b, context_length, d_model)\n",
    "    return F_QKV_linear_proj + F_attention + F_softmax + F_weighted + F_out_proj\n",
    "\n",
    "def get_qkv_proj(b, context_length, d_model):\n",
    "    return 3 * (2 * b * context_length * (d_model**2))\n",
    "\n",
    "def get_flops_atten_scores(b, context_length, d_model):\n",
    "    return 2 * b * (context_length**2) * d_model\n",
    "\n",
    "def get_flops_Softmax(b, context_length, num_heads):\n",
    "    return 5 * b * (context_length**2) * num_heads\n",
    "\n",
    "def get_flops_output_proj(b, context_length, d_model):\n",
    "    return 2 * b * context_length * d_model * d_model\n",
    "\n",
    "def get_flops_FFN(b, context_length, d_model, d_ff):\n",
    "    F_w1 = 2 * b * context_length * d_model * d_ff\n",
    "    F_gelu = get_flops_GELU(b, context_length, d_ff)\n",
    "    F_w2 = F_w1\n",
    "    return F_w1 + F_gelu + F_w2\n",
    "\n",
    "def get_flops_GELU(b, context_length, d_ff):\n",
    "    return 5 * b * context_length * d_ff\n",
    "\n",
    "def get_table_all_flops(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b = 1\n",
    "):\n",
    "    all_model_flops = {\n",
    "        \"Transformer_block\": {\n",
    "            \"RMSNorm_1\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "            \"MultiHeadAttentio\": {\n",
    "                \"QKV_proj\": get_qkv_proj(b, context_length, d_model),\n",
    "                \"Attention_scores\": get_flops_atten_scores(b, context_length, d_model),\n",
    "                \"Softmax\": get_flops_Softmax(b, context_length, num_heads),\n",
    "                \"Attention_weighted\": get_flops_atten_scores(b, context_length, d_model),\n",
    "                \"O_proj\": get_flops_output_proj(b, context_length, d_model),\n",
    "                \"Overall_MultiHeadAttentio\": get_flops_MultiHeadAttention(b, context_length, d_model, num_heads)\n",
    "            },\n",
    "            \"RMSNorm_2\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "            \"FFN\": get_flops_FFN(b, context_length, d_model, d_ff),\n",
    "            \"Overall_Transformer_block\": get_flops_TransformerBlock(\n",
    "                b, context_length, d_model, num_heads, d_ff\n",
    "            ) * num_layers\n",
    "        },\n",
    "        \"RMSNorm_final\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "        \"LM_head\": get_flops_lm_head(b, context_length, d_model, vocab_size),\n",
    "        \"forward_pass_TransformerLM\": get_forward_flops_TransformerLM(\n",
    "            vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "        ),\n",
    "        \"backward_pass_TransformerLM\": get_backward_flops_TransformerLM(\n",
    "            get_forward_flops_TransformerLM(\n",
    "                vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b)\n",
    "        ),\n",
    "        \"AdamW-step\": get_adamw_flops(\n",
    "        vocab_size, context_length, num_layers, d_model, d_ff\n",
    "        ),\n",
    "        \"Overall_TransformerLM\": get_overall_flops(\n",
    "            vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "        )\n",
    "    }\n",
    "    return all_model_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>15728640000</td>\n",
       "      <td>1.57e+10</td>\n",
       "      <td>1.487859e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.174098e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>131072000</td>\n",
       "      <td>1.31e+08</td>\n",
       "      <td>1.239882e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.174098e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>5242880000</td>\n",
       "      <td>5.24e+09</td>\n",
       "      <td>4.959529e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>27813478400</td>\n",
       "      <td>2.78e+10</td>\n",
       "      <td>2.631030e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>41975808000</td>\n",
       "      <td>4.20e+10</td>\n",
       "      <td>3.970723e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>3350357803008</td>\n",
       "      <td>3.35e+12</td>\n",
       "      <td>3.169288e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>164682137600</td>\n",
       "      <td>1.65e+11</td>\n",
       "      <td>1.557819e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3515044857856</td>\n",
       "      <td>3.52e+12</td>\n",
       "      <td>3.325074e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>7030089715712</td>\n",
       "      <td>7.03e+12</td>\n",
       "      <td>6.650149e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>26192332800</td>\n",
       "      <td>2.62e+10</td>\n",
       "      <td>2.477677e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>10571326906368</td>\n",
       "      <td>1.06e+13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                     \n",
       "Transformer_block           RMSNorm_1                                                   4917248   \n",
       "                            MultiHeadAttentio         QKV_proj                      15728640000   \n",
       "                                                      Attention_scores               3355443200   \n",
       "                                                      Softmax                         131072000   \n",
       "                                                      Attention_weighted             3355443200   \n",
       "                                                      O_proj                         5242880000   \n",
       "                                                      Overall_MultiHeadAttentio     27813478400   \n",
       "                            RMSNorm_2                                                   4917248   \n",
       "                            FFN                                                     41975808000   \n",
       "                            Overall_Transformer_block                             3350357803008   \n",
       "RMSNorm_final                                                                           4917248   \n",
       "LM_head                                                                            164682137600   \n",
       "forward_pass_TransformerLM                                                        3515044857856   \n",
       "backward_pass_TransformerLM                                                       7030089715712   \n",
       "AdamW-step                                                                          26192332800   \n",
       "Overall_TransformerLM                                                            10571326906368   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            4.92e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.57e+10   \n",
       "                                                      Attention_scores           3.36e+09   \n",
       "                                                      Softmax                    1.31e+08   \n",
       "                                                      Attention_weighted         3.36e+09   \n",
       "                                                      O_proj                     5.24e+09   \n",
       "                                                      Overall_MultiHeadAttentio  2.78e+10   \n",
       "                            RMSNorm_2                                            4.92e+06   \n",
       "                            FFN                                                  4.20e+10   \n",
       "                            Overall_Transformer_block                            3.35e+12   \n",
       "RMSNorm_final                                                                    4.92e+06   \n",
       "LM_head                                                                          1.65e+11   \n",
       "forward_pass_TransformerLM                                                       3.52e+12   \n",
       "backward_pass_TransformerLM                                                      7.03e+12   \n",
       "AdamW-step                                                                       2.62e+10   \n",
       "Overall_TransformerLM                                                            1.06e+13   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                4.651496e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.487859e-03  \n",
       "                                                      Attention_scores               3.174098e-04  \n",
       "                                                      Softmax                        1.239882e-05  \n",
       "                                                      Attention_weighted             3.174098e-04  \n",
       "                                                      O_proj                         4.959529e-04  \n",
       "                                                      Overall_MultiHeadAttentio      2.631030e-03  \n",
       "                            RMSNorm_2                                                4.651496e-07  \n",
       "                            FFN                                                      3.970723e-03  \n",
       "                            Overall_Transformer_block                                3.169288e-01  \n",
       "RMSNorm_final                                                                        4.651496e-07  \n",
       "LM_head                                                                              1.557819e-02  \n",
       "forward_pass_TransformerLM                                                           3.325074e-01  \n",
       "backward_pass_TransformerLM                                                          6.650149e-01  \n",
       "AdamW-step                                                                           2.477677e-03  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- your FLOP-computing helpers remain unchanged here ------------------ #\n",
    "# (get_flops_RMSNorm, get_flops_MultiHeadAttention, … get_table_all_flops)\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "def explode_to_tuples(d, prefix=()):\n",
    "    \"\"\"Yield (path_tuple, value) pairs from a nested dict.\"\"\"\n",
    "    for k, v in d.items():\n",
    "        path = prefix + (k,)\n",
    "        if isinstance(v, dict):\n",
    "            yield from explode_to_tuples(v, path)\n",
    "        else:\n",
    "            yield path, v\n",
    "\n",
    "def flops_table_multiindex(\n",
    "    vocab_size, context_len, n_layers, d_model, n_heads, d_ff, b=1\n",
    "):\n",
    "    # 1. get the nested FLOPs dict\n",
    "    nested = get_table_all_flops(\n",
    "        vocab_size, context_len, n_layers, d_model, n_heads, d_ff, b\n",
    "    )\n",
    "    overll_flops = nested['Overall_TransformerLM']\n",
    "    # 2. explode to (tuple, value) pairs\n",
    "    tuples, vals = zip(*explode_to_tuples(nested))\n",
    "\n",
    "    # 3. make all tuples the same length\n",
    "    max_depth = max(len(t) for t in tuples)\n",
    "    padded = [t + (\"\",) * (max_depth - len(t)) for t in tuples]\n",
    "\n",
    "    # 4. build the MultiIndex DataFrame\n",
    "    names = [f\"Level-{i+1}\" for i in range(max_depth)]\n",
    "    mi = pd.MultiIndex.from_tuples(padded, names=names)\n",
    "    df = pd.DataFrame({\"FLOPs\": vals}, index=mi)\n",
    "\n",
    "    # 5. optional pretty formatting\n",
    "    df[\"FLOPs_fmt\"] = df[\"FLOPs\"].astype(float).map(\"{:.2e}\".format)\n",
    "    df[\"FLOPs_proportion\"] = df[\"FLOPs\"] / overll_flops\n",
    "    return df\n",
    "\n",
    "\n",
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>3623878656</td>\n",
       "      <td>3.62e+09</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>1610612736</td>\n",
       "      <td>1.61e+09</td>\n",
       "      <td>0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>62914560</td>\n",
       "      <td>6.29e+07</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>1610612736</td>\n",
       "      <td>1.61e+09</td>\n",
       "      <td>0.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>1207959552</td>\n",
       "      <td>1.21e+09</td>\n",
       "      <td>0.000959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>8115978240</td>\n",
       "      <td>8.12e+09</td>\n",
       "      <td>0.006446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>20165427200</td>\n",
       "      <td>2.02e+10</td>\n",
       "      <td>0.016017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>339433537536</td>\n",
       "      <td>3.39e+11</td>\n",
       "      <td>0.269598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>79047426048</td>\n",
       "      <td>7.90e+10</td>\n",
       "      <td>0.062784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>418483324928</td>\n",
       "      <td>4.18e+11</td>\n",
       "      <td>0.332383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>836966649856</td>\n",
       "      <td>8.37e+11</td>\n",
       "      <td>0.664767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3588120576</td>\n",
       "      <td>3.59e+09</td>\n",
       "      <td>0.002850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>1259038095360</td>\n",
       "      <td>1.26e+12</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  2361344   \n",
       "                            MultiHeadAttentio         QKV_proj                      3623878656   \n",
       "                                                      Attention_scores              1610612736   \n",
       "                                                      Softmax                         62914560   \n",
       "                                                      Attention_weighted            1610612736   \n",
       "                                                      O_proj                        1207959552   \n",
       "                                                      Overall_MultiHeadAttentio     8115978240   \n",
       "                            RMSNorm_2                                                  2361344   \n",
       "                            FFN                                                    20165427200   \n",
       "                            Overall_Transformer_block                             339433537536   \n",
       "RMSNorm_final                                                                          2361344   \n",
       "LM_head                                                                            79047426048   \n",
       "forward_pass_TransformerLM                                                        418483324928   \n",
       "backward_pass_TransformerLM                                                       836966649856   \n",
       "AdamW-step                                                                          3588120576   \n",
       "Overall_TransformerLM                                                            1259038095360   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            2.36e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   3.62e+09   \n",
       "                                                      Attention_scores           1.61e+09   \n",
       "                                                      Softmax                    6.29e+07   \n",
       "                                                      Attention_weighted         1.61e+09   \n",
       "                                                      O_proj                     1.21e+09   \n",
       "                                                      Overall_MultiHeadAttentio  8.12e+09   \n",
       "                            RMSNorm_2                                            2.36e+06   \n",
       "                            FFN                                                  2.02e+10   \n",
       "                            Overall_Transformer_block                            3.39e+11   \n",
       "RMSNorm_final                                                                    2.36e+06   \n",
       "LM_head                                                                          7.90e+10   \n",
       "forward_pass_TransformerLM                                                       4.18e+11   \n",
       "backward_pass_TransformerLM                                                      8.37e+11   \n",
       "AdamW-step                                                                       3.59e+09   \n",
       "Overall_TransformerLM                                                            1.26e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                    0.000002  \n",
       "                            MultiHeadAttentio         QKV_proj                           0.002878  \n",
       "                                                      Attention_scores                   0.001279  \n",
       "                                                      Softmax                            0.000050  \n",
       "                                                      Attention_weighted                 0.001279  \n",
       "                                                      O_proj                             0.000959  \n",
       "                                                      Overall_MultiHeadAttentio          0.006446  \n",
       "                            RMSNorm_2                                                    0.000002  \n",
       "                            FFN                                                          0.016017  \n",
       "                            Overall_Transformer_block                                    0.269598  \n",
       "RMSNorm_final                                                                            0.000002  \n",
       "LM_head                                                                                  0.062784  \n",
       "forward_pass_TransformerLM                                                               0.332383  \n",
       "backward_pass_TransformerLM                                                              0.664767  \n",
       "AdamW-step                                                                               0.002850  \n",
       "Overall_TransformerLM                                                                    1.000000  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=12,\n",
    "    d_model=768,\n",
    "    n_heads=12,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.855874e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>6442450944</td>\n",
       "      <td>6.44e+09</td>\n",
       "      <td>2.017170e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.723899e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>83886080</td>\n",
       "      <td>8.39e+07</td>\n",
       "      <td>2.626523e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.723899e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.723899e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>12968787968</td>\n",
       "      <td>1.30e+10</td>\n",
       "      <td>4.060605e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.855874e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>26876313600</td>\n",
       "      <td>2.69e+10</td>\n",
       "      <td>8.415134e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>956433530880</td>\n",
       "      <td>9.56e+11</td>\n",
       "      <td>2.994650e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.855874e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>105396568064</td>\n",
       "      <td>1.05e+11</td>\n",
       "      <td>3.300029e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>1061833246720</td>\n",
       "      <td>1.06e+12</td>\n",
       "      <td>3.324663e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2123666493440</td>\n",
       "      <td>2.12e+12</td>\n",
       "      <td>6.649326e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>8307376128</td>\n",
       "      <td>8.31e+09</td>\n",
       "      <td>2.601089e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3193807116288</td>\n",
       "      <td>3.19e+12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  3147776   \n",
       "                            MultiHeadAttentio         QKV_proj                      6442450944   \n",
       "                                                      Attention_scores              2147483648   \n",
       "                                                      Softmax                         83886080   \n",
       "                                                      Attention_weighted            2147483648   \n",
       "                                                      O_proj                        2147483648   \n",
       "                                                      Overall_MultiHeadAttentio    12968787968   \n",
       "                            RMSNorm_2                                                  3147776   \n",
       "                            FFN                                                    26876313600   \n",
       "                            Overall_Transformer_block                             956433530880   \n",
       "RMSNorm_final                                                                          3147776   \n",
       "LM_head                                                                           105396568064   \n",
       "forward_pass_TransformerLM                                                       1061833246720   \n",
       "backward_pass_TransformerLM                                                      2123666493440   \n",
       "AdamW-step                                                                          8307376128   \n",
       "Overall_TransformerLM                                                            3193807116288   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            3.15e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   6.44e+09   \n",
       "                                                      Attention_scores           2.15e+09   \n",
       "                                                      Softmax                    8.39e+07   \n",
       "                                                      Attention_weighted         2.15e+09   \n",
       "                                                      O_proj                     2.15e+09   \n",
       "                                                      Overall_MultiHeadAttentio  1.30e+10   \n",
       "                            RMSNorm_2                                            3.15e+06   \n",
       "                            FFN                                                  2.69e+10   \n",
       "                            Overall_Transformer_block                            9.56e+11   \n",
       "RMSNorm_final                                                                    3.15e+06   \n",
       "LM_head                                                                          1.05e+11   \n",
       "forward_pass_TransformerLM                                                       1.06e+12   \n",
       "backward_pass_TransformerLM                                                      2.12e+12   \n",
       "AdamW-step                                                                       8.31e+09   \n",
       "Overall_TransformerLM                                                            3.19e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                9.855874e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       2.017170e-03  \n",
       "                                                      Attention_scores               6.723899e-04  \n",
       "                                                      Softmax                        2.626523e-05  \n",
       "                                                      Attention_weighted             6.723899e-04  \n",
       "                                                      O_proj                         6.723899e-04  \n",
       "                                                      Overall_MultiHeadAttentio      4.060605e-03  \n",
       "                            RMSNorm_2                                                9.855874e-07  \n",
       "                            FFN                                                      8.415134e-03  \n",
       "                            Overall_Transformer_block                                2.994650e-01  \n",
       "RMSNorm_final                                                                        9.855874e-07  \n",
       "LM_head                                                                              3.300029e-02  \n",
       "forward_pass_TransformerLM                                                           3.324663e-01  \n",
       "backward_pass_TransformerLM                                                          6.649326e-01  \n",
       "AdamW-step                                                                           2.601089e-03  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_medium = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=24,\n",
    "    d_model=1024,\n",
    "    n_heads=16,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.471264e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>10066329600</td>\n",
       "      <td>1.01e+10</td>\n",
       "      <td>1.655781e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>2684354560</td>\n",
       "      <td>2.68e+09</td>\n",
       "      <td>4.415417e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>104857600</td>\n",
       "      <td>1.05e+08</td>\n",
       "      <td>1.724772e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>2684354560</td>\n",
       "      <td>2.68e+09</td>\n",
       "      <td>4.415417e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>5.519271e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>18895339520</td>\n",
       "      <td>1.89e+10</td>\n",
       "      <td>3.108039e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.471264e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>33587200000</td>\n",
       "      <td>3.36e+10</td>\n",
       "      <td>5.524661e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>1889654685696</td>\n",
       "      <td>1.89e+12</td>\n",
       "      <td>3.108238e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.471264e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>131745710080</td>\n",
       "      <td>1.32e+11</td>\n",
       "      <td>2.167047e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2021404329984</td>\n",
       "      <td>2.02e+12</td>\n",
       "      <td>3.324949e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4042808659968</td>\n",
       "      <td>4.04e+12</td>\n",
       "      <td>6.649898e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>15291555840</td>\n",
       "      <td>1.53e+10</td>\n",
       "      <td>2.515263e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>6079504545792</td>\n",
       "      <td>6.08e+12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  3934208   \n",
       "                            MultiHeadAttentio         QKV_proj                     10066329600   \n",
       "                                                      Attention_scores              2684354560   \n",
       "                                                      Softmax                        104857600   \n",
       "                                                      Attention_weighted            2684354560   \n",
       "                                                      O_proj                        3355443200   \n",
       "                                                      Overall_MultiHeadAttentio    18895339520   \n",
       "                            RMSNorm_2                                                  3934208   \n",
       "                            FFN                                                    33587200000   \n",
       "                            Overall_Transformer_block                            1889654685696   \n",
       "RMSNorm_final                                                                          3934208   \n",
       "LM_head                                                                           131745710080   \n",
       "forward_pass_TransformerLM                                                       2021404329984   \n",
       "backward_pass_TransformerLM                                                      4042808659968   \n",
       "AdamW-step                                                                         15291555840   \n",
       "Overall_TransformerLM                                                            6079504545792   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            3.93e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.01e+10   \n",
       "                                                      Attention_scores           2.68e+09   \n",
       "                                                      Softmax                    1.05e+08   \n",
       "                                                      Attention_weighted         2.68e+09   \n",
       "                                                      O_proj                     3.36e+09   \n",
       "                                                      Overall_MultiHeadAttentio  1.89e+10   \n",
       "                            RMSNorm_2                                            3.93e+06   \n",
       "                            FFN                                                  3.36e+10   \n",
       "                            Overall_Transformer_block                            1.89e+12   \n",
       "RMSNorm_final                                                                    3.93e+06   \n",
       "LM_head                                                                          1.32e+11   \n",
       "forward_pass_TransformerLM                                                       2.02e+12   \n",
       "backward_pass_TransformerLM                                                      4.04e+12   \n",
       "AdamW-step                                                                       1.53e+10   \n",
       "Overall_TransformerLM                                                            6.08e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                6.471264e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.655781e-03  \n",
       "                                                      Attention_scores               4.415417e-04  \n",
       "                                                      Softmax                        1.724772e-05  \n",
       "                                                      Attention_weighted             4.415417e-04  \n",
       "                                                      O_proj                         5.519271e-04  \n",
       "                                                      Overall_MultiHeadAttentio      3.108039e-03  \n",
       "                            RMSNorm_2                                                6.471264e-07  \n",
       "                            FFN                                                      5.524661e-03  \n",
       "                            Overall_Transformer_block                                3.108238e-01  \n",
       "RMSNorm_final                                                                        6.471264e-07  \n",
       "LM_head                                                                              2.167047e-02  \n",
       "forward_pass_TransformerLM                                                           3.324949e-01  \n",
       "backward_pass_TransformerLM                                                          6.649898e-01  \n",
       "AdamW-step                                                                           2.515263e-03  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_large = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=36,\n",
    "    d_model=1280,\n",
    "    n_heads=20,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>15728640000</td>\n",
       "      <td>1.57e+10</td>\n",
       "      <td>1.487859e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.174098e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>131072000</td>\n",
       "      <td>1.31e+08</td>\n",
       "      <td>1.239882e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.174098e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>5242880000</td>\n",
       "      <td>5.24e+09</td>\n",
       "      <td>4.959529e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>27813478400</td>\n",
       "      <td>2.78e+10</td>\n",
       "      <td>2.631030e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>41975808000</td>\n",
       "      <td>4.20e+10</td>\n",
       "      <td>3.970723e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>3350357803008</td>\n",
       "      <td>3.35e+12</td>\n",
       "      <td>3.169288e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.651496e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>164682137600</td>\n",
       "      <td>1.65e+11</td>\n",
       "      <td>1.557819e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3515044857856</td>\n",
       "      <td>3.52e+12</td>\n",
       "      <td>3.325074e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>7030089715712</td>\n",
       "      <td>7.03e+12</td>\n",
       "      <td>6.650149e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>26192332800</td>\n",
       "      <td>2.62e+10</td>\n",
       "      <td>2.477677e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>10571326906368</td>\n",
       "      <td>1.06e+13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                     \n",
       "Transformer_block           RMSNorm_1                                                   4917248   \n",
       "                            MultiHeadAttentio         QKV_proj                      15728640000   \n",
       "                                                      Attention_scores               3355443200   \n",
       "                                                      Softmax                         131072000   \n",
       "                                                      Attention_weighted             3355443200   \n",
       "                                                      O_proj                         5242880000   \n",
       "                                                      Overall_MultiHeadAttentio     27813478400   \n",
       "                            RMSNorm_2                                                   4917248   \n",
       "                            FFN                                                     41975808000   \n",
       "                            Overall_Transformer_block                             3350357803008   \n",
       "RMSNorm_final                                                                           4917248   \n",
       "LM_head                                                                            164682137600   \n",
       "forward_pass_TransformerLM                                                        3515044857856   \n",
       "backward_pass_TransformerLM                                                       7030089715712   \n",
       "AdamW-step                                                                          26192332800   \n",
       "Overall_TransformerLM                                                            10571326906368   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            4.92e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.57e+10   \n",
       "                                                      Attention_scores           3.36e+09   \n",
       "                                                      Softmax                    1.31e+08   \n",
       "                                                      Attention_weighted         3.36e+09   \n",
       "                                                      O_proj                     5.24e+09   \n",
       "                                                      Overall_MultiHeadAttentio  2.78e+10   \n",
       "                            RMSNorm_2                                            4.92e+06   \n",
       "                            FFN                                                  4.20e+10   \n",
       "                            Overall_Transformer_block                            3.35e+12   \n",
       "RMSNorm_final                                                                    4.92e+06   \n",
       "LM_head                                                                          1.65e+11   \n",
       "forward_pass_TransformerLM                                                       3.52e+12   \n",
       "backward_pass_TransformerLM                                                      7.03e+12   \n",
       "AdamW-step                                                                       2.62e+10   \n",
       "Overall_TransformerLM                                                            1.06e+13   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                4.651496e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.487859e-03  \n",
       "                                                      Attention_scores               3.174098e-04  \n",
       "                                                      Softmax                        1.239882e-05  \n",
       "                                                      Attention_weighted             3.174098e-04  \n",
       "                                                      O_proj                         4.959529e-04  \n",
       "                                                      Overall_MultiHeadAttentio      2.631030e-03  \n",
       "                            RMSNorm_2                                                4.651496e-07  \n",
       "                            FFN                                                      3.970723e-03  \n",
       "                            Overall_Transformer_block                                3.169288e-01  \n",
       "RMSNorm_final                                                                        4.651496e-07  \n",
       "LM_head                                                                              1.557819e-02  \n",
       "forward_pass_TransformerLM                                                           3.325074e-01  \n",
       "backward_pass_TransformerLM                                                          6.650149e-01  \n",
       "AdamW-step                                                                           2.477677e-03  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941625e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>251658240000</td>\n",
       "      <td>2.52e+11</td>\n",
       "      <td>6.210612e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>858993459200</td>\n",
       "      <td>8.59e+11</td>\n",
       "      <td>2.119889e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>33554432000</td>\n",
       "      <td>3.36e+10</td>\n",
       "      <td>8.280816e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>858993459200</td>\n",
       "      <td>8.59e+11</td>\n",
       "      <td>2.119889e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>83886080000</td>\n",
       "      <td>8.39e+10</td>\n",
       "      <td>2.070204e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>2087085670400</td>\n",
       "      <td>2.09e+12</td>\n",
       "      <td>5.150668e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941625e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>671612928000</td>\n",
       "      <td>6.72e+11</td>\n",
       "      <td>1.657457e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>132425085616128</td>\n",
       "      <td>1.32e+14</td>\n",
       "      <td>3.268086e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941625e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2634914201600</td>\n",
       "      <td>2.63e+12</td>\n",
       "      <td>6.502640e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>135060078493696</td>\n",
       "      <td>1.35e+14</td>\n",
       "      <td>3.333115e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>270120156987392</td>\n",
       "      <td>2.70e+14</td>\n",
       "      <td>6.666229e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW-step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>26585548800</td>\n",
       "      <td>2.66e+10</td>\n",
       "      <td>6.560983e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>405206821029888</td>\n",
       "      <td>4.05e+14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                   78675968   \n",
       "                            MultiHeadAttentio         QKV_proj                      251658240000   \n",
       "                                                      Attention_scores              858993459200   \n",
       "                                                      Softmax                        33554432000   \n",
       "                                                      Attention_weighted            858993459200   \n",
       "                                                      O_proj                         83886080000   \n",
       "                                                      Overall_MultiHeadAttentio    2087085670400   \n",
       "                            RMSNorm_2                                                   78675968   \n",
       "                            FFN                                                     671612928000   \n",
       "                            Overall_Transformer_block                            132425085616128   \n",
       "RMSNorm_final                                                                           78675968   \n",
       "LM_head                                                                            2634914201600   \n",
       "forward_pass_TransformerLM                                                       135060078493696   \n",
       "backward_pass_TransformerLM                                                      270120156987392   \n",
       "AdamW-step                                                                           26585548800   \n",
       "Overall_TransformerLM                                                            405206821029888   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            7.87e+07   \n",
       "                            MultiHeadAttentio         QKV_proj                   2.52e+11   \n",
       "                                                      Attention_scores           8.59e+11   \n",
       "                                                      Softmax                    3.36e+10   \n",
       "                                                      Attention_weighted         8.59e+11   \n",
       "                                                      O_proj                     8.39e+10   \n",
       "                                                      Overall_MultiHeadAttentio  2.09e+12   \n",
       "                            RMSNorm_2                                            7.87e+07   \n",
       "                            FFN                                                  6.72e+11   \n",
       "                            Overall_Transformer_block                            1.32e+14   \n",
       "RMSNorm_final                                                                    7.87e+07   \n",
       "LM_head                                                                          2.63e+12   \n",
       "forward_pass_TransformerLM                                                       1.35e+14   \n",
       "backward_pass_TransformerLM                                                      2.70e+14   \n",
       "AdamW-step                                                                       2.66e+10   \n",
       "Overall_TransformerLM                                                            4.05e+14   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                1.941625e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       6.210612e-04  \n",
       "                                                      Attention_scores               2.119889e-03  \n",
       "                                                      Softmax                        8.280816e-05  \n",
       "                                                      Attention_weighted             2.119889e-03  \n",
       "                                                      O_proj                         2.070204e-04  \n",
       "                                                      Overall_MultiHeadAttentio      5.150668e-03  \n",
       "                            RMSNorm_2                                                1.941625e-07  \n",
       "                            FFN                                                      1.657457e-03  \n",
       "                            Overall_Transformer_block                                3.268086e-01  \n",
       "RMSNorm_final                                                                        1.941625e-07  \n",
       "LM_head                                                                              6.502640e-03  \n",
       "forward_pass_TransformerLM                                                           3.333115e-01  \n",
       "backward_pass_TransformerLM                                                          6.666229e-01  \n",
       "AdamW-step                                                                           6.560983e-05  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=16384,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much peak memory does running AdamW require? Decompose your answer based on the memory usage of the parameters, activations, gradients, and optimizer state. Express your answer\n",
    "in terms of the batch_size and the model hyperparameters (vocab_size, context_length,\n",
    "num_layers, d_model, num_heads). Assume d_ff = 4 ×d_model.\n",
    "For simplicity, when calculating memory usage of activations, consider only the following com-\n",
    "ponents:\n",
    "* Transformer block\n",
    "    - RMSNorm(s)\n",
    "    - Multi-head self-attention sublayer: QKV projections, QKT matrix multiply, softmax, weighted sum of values, output projection.\n",
    "    - Position-wise feed-forward: W1 matrix multiply, GELU, W2 matrix multiply\n",
    "* final RMSNorm\n",
    "* output embedding\n",
    "* cross-entropy on logits\n",
    "Deliverable: An algebraic expression for each of parameters, activations, gradients, and opti-\n",
    "mizer state, as well as the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constant_params(\n",
    "    vocab_size,\n",
    "    context_len,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    d_ff\n",
    "):\n",
    "  token_embeddings = vocab_size * d_model\n",
    "  position_embeddings = context_len * d_model\n",
    "  embd_layer = token_embeddings + position_embeddings\n",
    "\n",
    "  # Transformer parameter memory:\n",
    "  q_k_v_o = 4 * d_model * d_model\n",
    "  ffn = 2 * d_model * d_ff\n",
    "  transformer_blocks = num_layers * (q_k_v_o + ffn)\n",
    "\n",
    "  # lm head parameter:\n",
    "  lm_head = d_model * vocab_size\n",
    "\n",
    "  # Total trainable parameters:\n",
    "  total_params = embd_layer + transformer_blocks + lm_head\n",
    "\n",
    "  return total_params\n",
    "\n",
    "def get_activations_per_seq(\n",
    "    vocab_size,\n",
    "    context_len,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "):\n",
    "  # RMSNorm\n",
    "  # Each token goes through this\n",
    "  rmsnorma_act = context_len * d_model\n",
    "\n",
    "  # Q, K, V\n",
    "  # q, k, v projections\n",
    "  qkv_act = 3 * context_len * d_model\n",
    "\n",
    "  # scaled dot product attention:\n",
    "  pre_softmax_act = num_heads * context_len * context_len\n",
    "\n",
    "  # Concatenate the heads and pass to the next sub-layer:\n",
    "  attn_weight_act = context_len * d_model\n",
    "\n",
    "  # FFN:\n",
    "  # for h1 = W1 @ x -> need to backprop through GELU\n",
    "  h1_act = context_len * d_ff\n",
    "  # for h2 = GELU(h1) -> need to backprop through W2\n",
    "  h2_act = context_len * d_ff\n",
    "  # total\n",
    "  ffn_act = h1_act + h2_act\n",
    "\n",
    "  # Total per-layer activations per sequence\n",
    "  per_layer_act = rmsnorma_act + qkv_act + pre_softmax_act + attn_weight_act + ffn_act\n",
    "\n",
    "  # Total for all layers, then add final layer head\n",
    "  transformer_activations = num_layers * per_layer_act\n",
    "\n",
    "  final_rmsnorm = context_len * d_model\n",
    "  logits = context_len * vocab_size\n",
    "\n",
    "  # Total activations per sequence\n",
    "  activations_per_sequence = transformer_activations + final_rmsnorm + logits\n",
    "  return activations_per_sequence\n",
    "\n",
    "\n",
    "def get_constant_memory(\n",
    "    vocab_size,\n",
    "    context_len,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    d_ff,\n",
    "    dtype_size\n",
    "  ):\n",
    "  # --- Parameter memory ---\n",
    "  total_params = get_constant_params(\n",
    "      vocab_size=vocab_size,\n",
    "      context_len=context_len,\n",
    "      num_layers=num_layers,\n",
    "      d_model=d_model,\n",
    "      d_ff=d_ff,\n",
    "  )\n",
    "\n",
    "  # --- Gradients ---:\n",
    "  gradients = total_params\n",
    "\n",
    "  # --- Optimizer state (m and v per parameter) --- \n",
    "  AdamW_state = 2 * total_params\n",
    "\n",
    "  # --- Constant Memory ---:\n",
    "  total_constant = total_params + gradients + AdamW_state\n",
    "  total_mem_in_bytes = total_constant * dtype_size\n",
    "  return total_mem_in_bytes\n",
    "\n",
    "def get_activations_memory(\n",
    "  vocab_size,\n",
    "  context_len,\n",
    "  num_layers,\n",
    "  d_model,\n",
    "  num_heads,\n",
    "  d_ff,\n",
    "  dtype_size\n",
    "  ):\n",
    "  # --- Activation per sequence ---\n",
    "  activations = get_activations_per_seq(\n",
    "      vocab_size=vocab_size,\n",
    "      context_len=context_len,\n",
    "      num_layers=num_layers,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      d_ff = d_ff,\n",
    "  )\n",
    "  total_mem_in_bytes = activations * dtype_size\n",
    "  return total_mem_in_bytes\n",
    "\n",
    "def calc_mem(\n",
    "    vocab_size,\n",
    "    context_len,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff = 4*d_model,\n",
    "    batch_size = 1,\n",
    "    dtype_size=4\n",
    "):\n",
    "\n",
    "  constant_memory = get_constant_memory(\n",
    "    vocab_size=vocab_size,\n",
    "    context_len=context_len,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    d_ff=d_ff,\n",
    "    dtype_size=dtype_size\n",
    "  )\n",
    "  \n",
    "  activation_memory = get_activations_memory(\n",
    "    vocab_size,\n",
    "    context_len,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    dtype_size\n",
    "  )\n",
    "\n",
    "  # --- Memory totals in bytes --- \n",
    "  total_mem = activation_memory * batch_size + constant_memory\n",
    "  constant_memory = constant_memory / (1024 ** 3)\n",
    "  activation_memory = activation_memory / (1024 ** 3)\n",
    "  total_mem_in_gib = total_mem / (1024 ** 3)\n",
    "\n",
    "  print(f\"Total Constant Memory: {constant_memory:.2f} GiB ({'fp32' if dtype_size==4 else 'bf16'})\")\n",
    "  print(f\"Total Activation Memory: {activation_memory:.2f} GiB ({'fp32' if dtype_size==4 else 'bf16'})\")\n",
    "  print(f\"Total Memory (a * batch_size + b): {total_mem_in_gib:.2f} GiB ({'fp32' if dtype_size==4 else 'bf16'})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Constant Memory: 12.20 GiB (bf16)\n",
      "Total Activation Memory: 4.35 GiB (bf16)\n",
      "Total Memory (a * batch_size + b): 16.54 GiB (bf16)\n"
     ]
    }
   ],
   "source": [
    "calc_mem(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    num_layers=48,\n",
    "    d_model=1600,\n",
    "    d_ff = 4*d_model,\n",
    "    num_heads=25,\n",
    "    batch_size=1,\n",
    "    dtype_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate your answer for a GPT-2 XL-shaped model to get an expression that only depends on\n",
    "the batch_size. What is the maximum batch size you can use and still fit within 80GB memory?\n",
    "Deliverable: An expression that looks like a ·batch_size + b for numerical values a, b, and a\n",
    "number representing the maximum batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Constant Memory: 12.20 GiB (bf16)\n",
      "Total Activation Memory: 4.35 GiB (bf16)\n",
      "Total Memory (a * batch_size + b): 77.40 GiB (bf16)\n"
     ]
    }
   ],
   "source": [
    "calc_mem(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    num_layers=48,\n",
    "    d_model=1600,\n",
    "    d_ff = 4*d_model,\n",
    "    num_heads=25,\n",
    "    batch_size=15,\n",
    "    dtype_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many FLOPs does running one step of AdamW take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0262 TeraFLOPs'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_flops = 16 * (\n",
    "    2 * vocab_size * d_model # tok embedding + lm head\n",
    "    + context_len * d_model # position embedding\n",
    "    + num_layers * (4 * d_model**2 + 2 * d_model * d_ff) # Attention + FFN\n",
    ")\n",
    "f\"{adam_flops/1e12:.4f} TeraFLOPs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model FLOPs utilization (MFU) is defined as the ratio of observed throughput (tokens per second)\n",
    "relative to the hardware’s theoretical peak FLOP throughput [Chowdhery et al., 2022]. An\n",
    "NVIDIA A100 GPU has a theoretical peak of 19.5 teraFLOP/s for float32 operations. Assuming\n",
    "you are able to get 50% MFU, how long would it take to train a GPT-2 XL for 400K steps and a\n",
    "batch size of 1024 on a single A100? Following Kaplan et al. [2020] and Hoffmann et al. [2022],\n",
    "assume that the backward pass has twice the FLOPs of the forward pass.\n",
    "\n",
    "The number of days training would take, with a brief justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transformer flops per step:\n",
      "1.0571 TeraFLOPs\n",
      "Total transformer flops over 400K steps:\n",
      "422,853.0763 TeraFLOPs\n",
      "Total training time with NVIDIA A100 GPU:\n",
      "5.02 days\n"
     ]
    }
   ],
   "source": [
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "total_transformer_flops_per_step = gpt2_xl[\"FLOPs\"][\"Overall_TransformerLM\"].values[0]\n",
    "print(\"Total transformer flops per step:\")\n",
    "print(f\"{total_transformer_flops_per_step / 10e12:.4f} TeraFLOPs\")\n",
    "print(\"Total transformer flops over 400K steps:\")\n",
    "total_transformer_flops = 400*1e3 * total_transformer_flops_per_step\n",
    "print(f\"{total_transformer_flops / 10e12:,.4f} TeraFLOPs\")\n",
    "print(\"Total training time with NVIDIA A100 GPU:\")\n",
    "seconds = total_transformer_flops / (19.5*1e12 * 0.5)\n",
    "days = seconds / (60 * 60 * 24)\n",
    "print(f\"{days:.2f} days\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
