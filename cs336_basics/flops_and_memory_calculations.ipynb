{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider GPT-2 XL, which has the following configuration:\n",
    "-  vocab_size: 50,257\n",
    "-   context_length: 1,024\n",
    "-   num_layers: 48\n",
    "-   d_model: 1,600\n",
    "-   num_heads: 25\n",
    "-   d_ff: 6,400\n",
    "\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters would our model have? Assuming each parameter is represented using singl-precision floating point, how much memory is required to just load this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params 1.64B\n",
      "Memory fp32: 6.10 GiB\n",
      "Memory bf16: 3.05 GiB\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400\n",
    "\n",
    "token_embeddings = vocab_size * d_model\n",
    "position_embeddings = context_length * d_model\n",
    "embd_layer = token_embeddings + position_embeddings\n",
    "\n",
    "q_k_v_o = 4 * d_model * d_model\n",
    "ffn = 2 * d_model * d_ff\n",
    "transformer_blocks = num_layers * (q_k_v_o + ffn)\n",
    "\n",
    "lm_head = d_model * vocab_size\n",
    "\n",
    "total_params = embd_layer + transformer_blocks + lm_head\n",
    "print(f\"Total trainable params {total_params/10**9:0.2f}B\")\n",
    "bytes_float32 = total_params * 4\n",
    "mem_fp32 = bytes_float32 / 1024 ** 3\n",
    "bytes_bf16 = total_params * 2\n",
    "mem_bf16 = bytes_bf16 / 1024 ** 3\n",
    "print(f\"Memory fp32: {mem_fp32:,.2f} GiB\")\n",
    "print(f\"Memory bf16: {mem_bf16:,.2f} GiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped model.\n",
    "How many FLOPs do these matrix multiplies require in total? Assume that our input\n",
    "sequence has `context_length` tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_flops(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b = 1\n",
    "):\n",
    "    F_forward_pass = get_forward_flops_TransformerLM(\n",
    "        vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "    )\n",
    "    F_backward_pass = get_backward_flops_TransformerLM(\n",
    "        F_forward_pass\n",
    "    )\n",
    "    return F_forward_pass + F_backward_pass\n",
    "\n",
    "def get_forward_flops_TransformerLM(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b = 1\n",
    "):\n",
    "    F_transformer_blocks = get_flops_TransformerBlock(\n",
    "        b, context_length, d_model, num_heads, d_ff\n",
    "    ) * num_layers\n",
    "    F_ln_final = get_flops_RMSNorm(b, context_length, d_model)\n",
    "    F_lm_head = get_flops_lm_head(b, context_length, d_model, vocab_size)\n",
    "    return F_transformer_blocks + F_ln_final + F_lm_head\n",
    "\n",
    "def get_backward_flops_TransformerLM(\n",
    "    F_forward_pass\n",
    "):\n",
    "    F_backward_pass = 2 * F_forward_pass\n",
    "    return F_backward_pass\n",
    "\n",
    "\n",
    "def get_flops_lm_head(b, context_length, d_model, vocab_size):\n",
    "    return 2 * b * context_length * d_model * vocab_size\n",
    "\n",
    "def get_flops_TransformerBlock(b, context_length, d_model, num_heads, d_ff):\n",
    "    F_rmsnorm_1 = get_flops_RMSNorm(b, context_length, d_model)\n",
    "    F_mha = get_flops_MultiHeadAttention(b, context_length, d_model, num_heads)\n",
    "    F_rmsnorm_2 = F_rmsnorm_1\n",
    "    F_ffn = get_flops_FFN(b, context_length, d_model, d_ff)\n",
    "    return F_rmsnorm_1 + F_mha + F_rmsnorm_2 + F_ffn\n",
    "\n",
    "def get_flops_RMSNorm(b, context_length, d_model):\n",
    "    return (3*d_model + 2) * b * context_length\n",
    "\n",
    "def get_flops_MultiHeadAttention(b, context_length, d_model, num_heads):\n",
    "    F_QKV_linear_proj = get_qkv_proj(b, context_length, d_model)\n",
    "    F_attention = get_flops_atten_scores(b, context_length, d_model)\n",
    "    F_softmax = get_flops_Softmax(b, context_length, num_heads)\n",
    "    F_weighted = get_flops_atten_scores(b, context_length, d_model)\n",
    "    F_out_proj = get_flops_output_proj(b, context_length, d_model)\n",
    "    return F_QKV_linear_proj + F_attention + F_softmax + F_weighted + F_out_proj\n",
    "\n",
    "def get_qkv_proj(b, context_length, d_model):\n",
    "    return 3 * (2 * b * context_length * (d_model**2))\n",
    "\n",
    "def get_flops_atten_scores(b, context_length, d_model):\n",
    "    return 2 * b * (context_length**2) * d_model\n",
    "\n",
    "def get_flops_Softmax(b, context_length, num_heads):\n",
    "    return 5 * b * (context_length**2) * num_heads\n",
    "\n",
    "def get_flops_output_proj(b, context_length, d_model):\n",
    "    return 2 * b * context_length * d_model * d_model\n",
    "\n",
    "def get_flops_FFN(b, context_length, d_model, d_ff):\n",
    "    F_w1 = 2 * b * context_length * d_model * d_ff\n",
    "    F_gelu = get_flops_GELU(b, context_length, d_ff)\n",
    "    F_w2 = F_w1\n",
    "    return F_w1 + F_gelu + F_w2\n",
    "\n",
    "def get_flops_GELU(b, context_length, d_ff):\n",
    "    return 5 * b * context_length * d_ff\n",
    "\n",
    "def get_table_all_flops(\n",
    "    vocab_size,\n",
    "    context_length,\n",
    "    num_layers,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    b = 1\n",
    "):\n",
    "    all_model_flops = {\n",
    "        \"Transformer_block\": {\n",
    "            \"RMSNorm_1\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "            \"MultiHeadAttentio\": {\n",
    "                \"QKV_proj\": get_qkv_proj(b, context_length, d_model),\n",
    "                \"Attention_scores\": get_flops_atten_scores(b, context_length, d_model),\n",
    "                \"Softmax\": get_flops_Softmax(b, context_length, num_heads),\n",
    "                \"Attention_weighted\": get_flops_atten_scores(b, context_length, d_model),\n",
    "                \"O_proj\": get_flops_output_proj(b, context_length, d_model),\n",
    "                \"Overall_MultiHeadAttentio\": get_flops_MultiHeadAttention(b, context_length, d_model, num_heads)\n",
    "            },\n",
    "            \"RMSNorm_2\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "            \"FFN\": get_flops_FFN(b, context_length, d_model, d_ff),\n",
    "            \"Overall_Transformer_block\": get_flops_TransformerBlock(\n",
    "                b, context_length, d_model, num_heads, d_ff\n",
    "            ) * num_layers\n",
    "        },\n",
    "        \"RMSNorm_final\": get_flops_RMSNorm(b, context_length, d_model),\n",
    "        \"LM_head\": get_flops_lm_head(b, context_length, d_model, vocab_size),\n",
    "        \"forward_pass_TransformerLM\": get_forward_flops_TransformerLM(\n",
    "            vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "        ),\n",
    "        \"backward_pass_TransformerLM\": get_backward_flops_TransformerLM(\n",
    "            get_forward_flops_TransformerLM(\n",
    "                vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b)\n",
    "        ),\n",
    "        \"Overall_TransformerLM\": get_overall_flops(\n",
    "            vocab_size, context_length, num_layers, d_model, num_heads, d_ff, b\n",
    "        )\n",
    "    }\n",
    "    return all_model_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>15728640000</td>\n",
       "      <td>1.57e+10</td>\n",
       "      <td>1.491554e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.181982e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>131072000</td>\n",
       "      <td>1.31e+08</td>\n",
       "      <td>1.242962e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.181982e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>5242880000</td>\n",
       "      <td>5.24e+09</td>\n",
       "      <td>4.971847e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>27813478400</td>\n",
       "      <td>2.78e+10</td>\n",
       "      <td>2.637565e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>41975808000</td>\n",
       "      <td>4.20e+10</td>\n",
       "      <td>3.980585e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>3350357803008</td>\n",
       "      <td>3.35e+12</td>\n",
       "      <td>3.177160e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>164682137600</td>\n",
       "      <td>1.65e+11</td>\n",
       "      <td>1.561688e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3515044857856</td>\n",
       "      <td>3.52e+12</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>7030089715712</td>\n",
       "      <td>7.03e+12</td>\n",
       "      <td>6.666667e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>10545134573568</td>\n",
       "      <td>1.05e+13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                     \n",
       "Transformer_block           RMSNorm_1                                                   4917248   \n",
       "                            MultiHeadAttentio         QKV_proj                      15728640000   \n",
       "                                                      Attention_scores               3355443200   \n",
       "                                                      Softmax                         131072000   \n",
       "                                                      Attention_weighted             3355443200   \n",
       "                                                      O_proj                         5242880000   \n",
       "                                                      Overall_MultiHeadAttentio     27813478400   \n",
       "                            RMSNorm_2                                                   4917248   \n",
       "                            FFN                                                     41975808000   \n",
       "                            Overall_Transformer_block                             3350357803008   \n",
       "RMSNorm_final                                                                           4917248   \n",
       "LM_head                                                                            164682137600   \n",
       "forward_pass_TransformerLM                                                        3515044857856   \n",
       "backward_pass_TransformerLM                                                       7030089715712   \n",
       "Overall_TransformerLM                                                            10545134573568   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            4.92e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.57e+10   \n",
       "                                                      Attention_scores           3.36e+09   \n",
       "                                                      Softmax                    1.31e+08   \n",
       "                                                      Attention_weighted         3.36e+09   \n",
       "                                                      O_proj                     5.24e+09   \n",
       "                                                      Overall_MultiHeadAttentio  2.78e+10   \n",
       "                            RMSNorm_2                                            4.92e+06   \n",
       "                            FFN                                                  4.20e+10   \n",
       "                            Overall_Transformer_block                            3.35e+12   \n",
       "RMSNorm_final                                                                    4.92e+06   \n",
       "LM_head                                                                          1.65e+11   \n",
       "forward_pass_TransformerLM                                                       3.52e+12   \n",
       "backward_pass_TransformerLM                                                      7.03e+12   \n",
       "Overall_TransformerLM                                                            1.05e+13   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                4.663049e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.491554e-03  \n",
       "                                                      Attention_scores               3.181982e-04  \n",
       "                                                      Softmax                        1.242962e-05  \n",
       "                                                      Attention_weighted             3.181982e-04  \n",
       "                                                      O_proj                         4.971847e-04  \n",
       "                                                      Overall_MultiHeadAttentio      2.637565e-03  \n",
       "                            RMSNorm_2                                                4.663049e-07  \n",
       "                            FFN                                                      3.980585e-03  \n",
       "                            Overall_Transformer_block                                3.177160e-01  \n",
       "RMSNorm_final                                                                        4.663049e-07  \n",
       "LM_head                                                                              1.561688e-02  \n",
       "forward_pass_TransformerLM                                                           3.333333e-01  \n",
       "backward_pass_TransformerLM                                                          6.666667e-01  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- your FLOP-computing helpers remain unchanged here ------------------ #\n",
    "# (get_flops_RMSNorm, get_flops_MultiHeadAttention, â€¦ get_table_all_flops)\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "def explode_to_tuples(d, prefix=()):\n",
    "    \"\"\"Yield (path_tuple, value) pairs from a nested dict.\"\"\"\n",
    "    for k, v in d.items():\n",
    "        path = prefix + (k,)\n",
    "        if isinstance(v, dict):\n",
    "            yield from explode_to_tuples(v, path)\n",
    "        else:\n",
    "            yield path, v\n",
    "\n",
    "def flops_table_multiindex(\n",
    "    vocab_size, context_len, n_layers, d_model, n_heads, d_ff, b=1\n",
    "):\n",
    "    # 1. get the nested FLOPs dict\n",
    "    nested = get_table_all_flops(\n",
    "        vocab_size, context_len, n_layers, d_model, n_heads, d_ff, b\n",
    "    )\n",
    "    overll_flops = nested['Overall_TransformerLM']\n",
    "    # 2. explode to (tuple, value) pairs\n",
    "    tuples, vals = zip(*explode_to_tuples(nested))\n",
    "\n",
    "    # 3. make all tuples the same length\n",
    "    max_depth = max(len(t) for t in tuples)\n",
    "    padded = [t + (\"\",) * (max_depth - len(t)) for t in tuples]\n",
    "\n",
    "    # 4. build the MultiIndex DataFrame\n",
    "    names = [f\"Level-{i+1}\" for i in range(max_depth)]\n",
    "    mi = pd.MultiIndex.from_tuples(padded, names=names)\n",
    "    df = pd.DataFrame({\"FLOPs\": vals}, index=mi)\n",
    "\n",
    "    # 5. optional pretty formatting\n",
    "    df[\"FLOPs_fmt\"] = df[\"FLOPs\"].astype(float).map(\"{:.2e}\".format)\n",
    "    df[\"FLOPs_proportion\"] = df[\"FLOPs\"] / overll_flops\n",
    "    return df\n",
    "\n",
    "\n",
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16323569411002548"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 6 * total_params * 50e3\n",
    "a = 6 * num_layers * d_model*d_model * context_length + 12 * num_layers * context_length * context_length  * d_model\n",
    "a / 10545134573568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>3623878656</td>\n",
       "      <td>3.62e+09</td>\n",
       "      <td>0.002887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>1610612736</td>\n",
       "      <td>1.61e+09</td>\n",
       "      <td>0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>62914560</td>\n",
       "      <td>6.29e+07</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>1610612736</td>\n",
       "      <td>1.61e+09</td>\n",
       "      <td>0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>1207959552</td>\n",
       "      <td>1.21e+09</td>\n",
       "      <td>0.000962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>8115978240</td>\n",
       "      <td>8.12e+09</td>\n",
       "      <td>0.006465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>20165427200</td>\n",
       "      <td>2.02e+10</td>\n",
       "      <td>0.016062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>339433537536</td>\n",
       "      <td>3.39e+11</td>\n",
       "      <td>0.270368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2361344</td>\n",
       "      <td>2.36e+06</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>79047426048</td>\n",
       "      <td>7.90e+10</td>\n",
       "      <td>0.062963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>418483324928</td>\n",
       "      <td>4.18e+11</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>836966649856</td>\n",
       "      <td>8.37e+11</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>1255449974784</td>\n",
       "      <td>1.26e+12</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  2361344   \n",
       "                            MultiHeadAttentio         QKV_proj                      3623878656   \n",
       "                                                      Attention_scores              1610612736   \n",
       "                                                      Softmax                         62914560   \n",
       "                                                      Attention_weighted            1610612736   \n",
       "                                                      O_proj                        1207959552   \n",
       "                                                      Overall_MultiHeadAttentio     8115978240   \n",
       "                            RMSNorm_2                                                  2361344   \n",
       "                            FFN                                                    20165427200   \n",
       "                            Overall_Transformer_block                             339433537536   \n",
       "RMSNorm_final                                                                          2361344   \n",
       "LM_head                                                                            79047426048   \n",
       "forward_pass_TransformerLM                                                        418483324928   \n",
       "backward_pass_TransformerLM                                                       836966649856   \n",
       "Overall_TransformerLM                                                            1255449974784   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            2.36e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   3.62e+09   \n",
       "                                                      Attention_scores           1.61e+09   \n",
       "                                                      Softmax                    6.29e+07   \n",
       "                                                      Attention_weighted         1.61e+09   \n",
       "                                                      O_proj                     1.21e+09   \n",
       "                                                      Overall_MultiHeadAttentio  8.12e+09   \n",
       "                            RMSNorm_2                                            2.36e+06   \n",
       "                            FFN                                                  2.02e+10   \n",
       "                            Overall_Transformer_block                            3.39e+11   \n",
       "RMSNorm_final                                                                    2.36e+06   \n",
       "LM_head                                                                          7.90e+10   \n",
       "forward_pass_TransformerLM                                                       4.18e+11   \n",
       "backward_pass_TransformerLM                                                      8.37e+11   \n",
       "Overall_TransformerLM                                                            1.26e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                    0.000002  \n",
       "                            MultiHeadAttentio         QKV_proj                           0.002887  \n",
       "                                                      Attention_scores                   0.001283  \n",
       "                                                      Softmax                            0.000050  \n",
       "                                                      Attention_weighted                 0.001283  \n",
       "                                                      O_proj                             0.000962  \n",
       "                                                      Overall_MultiHeadAttentio          0.006465  \n",
       "                            RMSNorm_2                                                    0.000002  \n",
       "                            FFN                                                          0.016062  \n",
       "                            Overall_Transformer_block                                    0.270368  \n",
       "RMSNorm_final                                                                            0.000002  \n",
       "LM_head                                                                                  0.062963  \n",
       "forward_pass_TransformerLM                                                               0.333333  \n",
       "backward_pass_TransformerLM                                                              0.666667  \n",
       "Overall_TransformerLM                                                                    1.000000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=12,\n",
    "    d_model=768,\n",
    "    n_heads=12,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.881577e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>6442450944</td>\n",
       "      <td>6.44e+09</td>\n",
       "      <td>2.022430e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.741434e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>83886080</td>\n",
       "      <td>8.39e+07</td>\n",
       "      <td>2.633373e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.741434e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>2147483648</td>\n",
       "      <td>2.15e+09</td>\n",
       "      <td>6.741434e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>12968787968</td>\n",
       "      <td>1.30e+10</td>\n",
       "      <td>4.071194e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.881577e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>26876313600</td>\n",
       "      <td>2.69e+10</td>\n",
       "      <td>8.437079e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>956433530880</td>\n",
       "      <td>9.56e+11</td>\n",
       "      <td>3.002460e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3147776</td>\n",
       "      <td>3.15e+06</td>\n",
       "      <td>9.881577e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>105396568064</td>\n",
       "      <td>1.05e+11</td>\n",
       "      <td>3.308635e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>1061833246720</td>\n",
       "      <td>1.06e+12</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2123666493440</td>\n",
       "      <td>2.12e+12</td>\n",
       "      <td>6.666667e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3185499740160</td>\n",
       "      <td>3.19e+12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  3147776   \n",
       "                            MultiHeadAttentio         QKV_proj                      6442450944   \n",
       "                                                      Attention_scores              2147483648   \n",
       "                                                      Softmax                         83886080   \n",
       "                                                      Attention_weighted            2147483648   \n",
       "                                                      O_proj                        2147483648   \n",
       "                                                      Overall_MultiHeadAttentio    12968787968   \n",
       "                            RMSNorm_2                                                  3147776   \n",
       "                            FFN                                                    26876313600   \n",
       "                            Overall_Transformer_block                             956433530880   \n",
       "RMSNorm_final                                                                          3147776   \n",
       "LM_head                                                                           105396568064   \n",
       "forward_pass_TransformerLM                                                       1061833246720   \n",
       "backward_pass_TransformerLM                                                      2123666493440   \n",
       "Overall_TransformerLM                                                            3185499740160   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            3.15e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   6.44e+09   \n",
       "                                                      Attention_scores           2.15e+09   \n",
       "                                                      Softmax                    8.39e+07   \n",
       "                                                      Attention_weighted         2.15e+09   \n",
       "                                                      O_proj                     2.15e+09   \n",
       "                                                      Overall_MultiHeadAttentio  1.30e+10   \n",
       "                            RMSNorm_2                                            3.15e+06   \n",
       "                            FFN                                                  2.69e+10   \n",
       "                            Overall_Transformer_block                            9.56e+11   \n",
       "RMSNorm_final                                                                    3.15e+06   \n",
       "LM_head                                                                          1.05e+11   \n",
       "forward_pass_TransformerLM                                                       1.06e+12   \n",
       "backward_pass_TransformerLM                                                      2.12e+12   \n",
       "Overall_TransformerLM                                                            3.19e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                9.881577e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       2.022430e-03  \n",
       "                                                      Attention_scores               6.741434e-04  \n",
       "                                                      Softmax                        2.633373e-05  \n",
       "                                                      Attention_weighted             6.741434e-04  \n",
       "                                                      O_proj                         6.741434e-04  \n",
       "                                                      Overall_MultiHeadAttentio      4.071194e-03  \n",
       "                            RMSNorm_2                                                9.881577e-07  \n",
       "                            FFN                                                      8.437079e-03  \n",
       "                            Overall_Transformer_block                                3.002460e-01  \n",
       "RMSNorm_final                                                                        9.881577e-07  \n",
       "LM_head                                                                              3.308635e-02  \n",
       "forward_pass_TransformerLM                                                           3.333333e-01  \n",
       "backward_pass_TransformerLM                                                          6.666667e-01  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_medium = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=24,\n",
    "    d_model=1024,\n",
    "    n_heads=16,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.487582e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>10066329600</td>\n",
       "      <td>1.01e+10</td>\n",
       "      <td>1.659956e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>2684354560</td>\n",
       "      <td>2.68e+09</td>\n",
       "      <td>4.426551e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>104857600</td>\n",
       "      <td>1.05e+08</td>\n",
       "      <td>1.729121e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>2684354560</td>\n",
       "      <td>2.68e+09</td>\n",
       "      <td>4.426551e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>5.533188e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>18895339520</td>\n",
       "      <td>1.89e+10</td>\n",
       "      <td>3.115877e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.487582e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>33587200000</td>\n",
       "      <td>3.36e+10</td>\n",
       "      <td>5.538592e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>1889654685696</td>\n",
       "      <td>1.89e+12</td>\n",
       "      <td>3.116076e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3934208</td>\n",
       "      <td>3.93e+06</td>\n",
       "      <td>6.487582e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>131745710080</td>\n",
       "      <td>1.32e+11</td>\n",
       "      <td>2.172511e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2021404329984</td>\n",
       "      <td>2.02e+12</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4042808659968</td>\n",
       "      <td>4.04e+12</td>\n",
       "      <td>6.666667e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>6064212989952</td>\n",
       "      <td>6.06e+12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                    \n",
       "Transformer_block           RMSNorm_1                                                  3934208   \n",
       "                            MultiHeadAttentio         QKV_proj                     10066329600   \n",
       "                                                      Attention_scores              2684354560   \n",
       "                                                      Softmax                        104857600   \n",
       "                                                      Attention_weighted            2684354560   \n",
       "                                                      O_proj                        3355443200   \n",
       "                                                      Overall_MultiHeadAttentio    18895339520   \n",
       "                            RMSNorm_2                                                  3934208   \n",
       "                            FFN                                                    33587200000   \n",
       "                            Overall_Transformer_block                            1889654685696   \n",
       "RMSNorm_final                                                                          3934208   \n",
       "LM_head                                                                           131745710080   \n",
       "forward_pass_TransformerLM                                                       2021404329984   \n",
       "backward_pass_TransformerLM                                                      4042808659968   \n",
       "Overall_TransformerLM                                                            6064212989952   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            3.93e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.01e+10   \n",
       "                                                      Attention_scores           2.68e+09   \n",
       "                                                      Softmax                    1.05e+08   \n",
       "                                                      Attention_weighted         2.68e+09   \n",
       "                                                      O_proj                     3.36e+09   \n",
       "                                                      Overall_MultiHeadAttentio  1.89e+10   \n",
       "                            RMSNorm_2                                            3.93e+06   \n",
       "                            FFN                                                  3.36e+10   \n",
       "                            Overall_Transformer_block                            1.89e+12   \n",
       "RMSNorm_final                                                                    3.93e+06   \n",
       "LM_head                                                                          1.32e+11   \n",
       "forward_pass_TransformerLM                                                       2.02e+12   \n",
       "backward_pass_TransformerLM                                                      4.04e+12   \n",
       "Overall_TransformerLM                                                            6.06e+12   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                6.487582e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.659956e-03  \n",
       "                                                      Attention_scores               4.426551e-04  \n",
       "                                                      Softmax                        1.729121e-05  \n",
       "                                                      Attention_weighted             4.426551e-04  \n",
       "                                                      O_proj                         5.533188e-04  \n",
       "                                                      Overall_MultiHeadAttentio      3.115877e-03  \n",
       "                            RMSNorm_2                                                6.487582e-07  \n",
       "                            FFN                                                      5.538592e-03  \n",
       "                            Overall_Transformer_block                                3.116076e-01  \n",
       "RMSNorm_final                                                                        6.487582e-07  \n",
       "LM_head                                                                              2.172511e-02  \n",
       "forward_pass_TransformerLM                                                           3.333333e-01  \n",
       "backward_pass_TransformerLM                                                          6.666667e-01  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_large = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=36,\n",
    "    d_model=1280,\n",
    "    n_heads=20,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>15728640000</td>\n",
       "      <td>1.57e+10</td>\n",
       "      <td>1.491554e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.181982e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>131072000</td>\n",
       "      <td>1.31e+08</td>\n",
       "      <td>1.242962e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>3355443200</td>\n",
       "      <td>3.36e+09</td>\n",
       "      <td>3.181982e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>5242880000</td>\n",
       "      <td>5.24e+09</td>\n",
       "      <td>4.971847e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>27813478400</td>\n",
       "      <td>2.78e+10</td>\n",
       "      <td>2.637565e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>41975808000</td>\n",
       "      <td>4.20e+10</td>\n",
       "      <td>3.980585e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>3350357803008</td>\n",
       "      <td>3.35e+12</td>\n",
       "      <td>3.177160e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>4917248</td>\n",
       "      <td>4.92e+06</td>\n",
       "      <td>4.663049e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>164682137600</td>\n",
       "      <td>1.65e+11</td>\n",
       "      <td>1.561688e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>3515044857856</td>\n",
       "      <td>3.52e+12</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>7030089715712</td>\n",
       "      <td>7.03e+12</td>\n",
       "      <td>6.666667e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>10545134573568</td>\n",
       "      <td>1.05e+13</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                     \n",
       "Transformer_block           RMSNorm_1                                                   4917248   \n",
       "                            MultiHeadAttentio         QKV_proj                      15728640000   \n",
       "                                                      Attention_scores               3355443200   \n",
       "                                                      Softmax                         131072000   \n",
       "                                                      Attention_weighted             3355443200   \n",
       "                                                      O_proj                         5242880000   \n",
       "                                                      Overall_MultiHeadAttentio     27813478400   \n",
       "                            RMSNorm_2                                                   4917248   \n",
       "                            FFN                                                     41975808000   \n",
       "                            Overall_Transformer_block                             3350357803008   \n",
       "RMSNorm_final                                                                           4917248   \n",
       "LM_head                                                                            164682137600   \n",
       "forward_pass_TransformerLM                                                        3515044857856   \n",
       "backward_pass_TransformerLM                                                       7030089715712   \n",
       "Overall_TransformerLM                                                            10545134573568   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            4.92e+06   \n",
       "                            MultiHeadAttentio         QKV_proj                   1.57e+10   \n",
       "                                                      Attention_scores           3.36e+09   \n",
       "                                                      Softmax                    1.31e+08   \n",
       "                                                      Attention_weighted         3.36e+09   \n",
       "                                                      O_proj                     5.24e+09   \n",
       "                                                      Overall_MultiHeadAttentio  2.78e+10   \n",
       "                            RMSNorm_2                                            4.92e+06   \n",
       "                            FFN                                                  4.20e+10   \n",
       "                            Overall_Transformer_block                            3.35e+12   \n",
       "RMSNorm_final                                                                    4.92e+06   \n",
       "LM_head                                                                          1.65e+11   \n",
       "forward_pass_TransformerLM                                                       3.52e+12   \n",
       "backward_pass_TransformerLM                                                      7.03e+12   \n",
       "Overall_TransformerLM                                                            1.05e+13   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                4.663049e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       1.491554e-03  \n",
       "                                                      Attention_scores               3.181982e-04  \n",
       "                                                      Softmax                        1.242962e-05  \n",
       "                                                      Attention_weighted             3.181982e-04  \n",
       "                                                      O_proj                         4.971847e-04  \n",
       "                                                      Overall_MultiHeadAttentio      2.637565e-03  \n",
       "                            RMSNorm_2                                                4.663049e-07  \n",
       "                            FFN                                                      3.980585e-03  \n",
       "                            Overall_Transformer_block                                3.177160e-01  \n",
       "RMSNorm_final                                                                        4.663049e-07  \n",
       "LM_head                                                                              1.561688e-02  \n",
       "forward_pass_TransformerLM                                                           3.333333e-01  \n",
       "backward_pass_TransformerLM                                                          6.666667e-01  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=1024,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FLOPs</th>\n",
       "      <th>FLOPs_fmt</th>\n",
       "      <th>FLOPs_proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Level-1</th>\n",
       "      <th>Level-2</th>\n",
       "      <th>Level-3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">Transformer_block</th>\n",
       "      <th>RMSNorm_1</th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941752e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">MultiHeadAttentio</th>\n",
       "      <th>QKV_proj</th>\n",
       "      <td>251658240000</td>\n",
       "      <td>2.52e+11</td>\n",
       "      <td>6.211020e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_scores</th>\n",
       "      <td>858993459200</td>\n",
       "      <td>8.59e+11</td>\n",
       "      <td>2.120028e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Softmax</th>\n",
       "      <td>33554432000</td>\n",
       "      <td>3.36e+10</td>\n",
       "      <td>8.281360e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention_weighted</th>\n",
       "      <td>858993459200</td>\n",
       "      <td>8.59e+11</td>\n",
       "      <td>2.120028e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O_proj</th>\n",
       "      <td>83886080000</td>\n",
       "      <td>8.39e+10</td>\n",
       "      <td>2.070340e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_MultiHeadAttentio</th>\n",
       "      <td>2087085670400</td>\n",
       "      <td>2.09e+12</td>\n",
       "      <td>5.151006e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_2</th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941752e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFN</th>\n",
       "      <th></th>\n",
       "      <td>671612928000</td>\n",
       "      <td>6.72e+11</td>\n",
       "      <td>1.657566e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_Transformer_block</th>\n",
       "      <th></th>\n",
       "      <td>132425085616128</td>\n",
       "      <td>1.32e+14</td>\n",
       "      <td>3.268301e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm_final</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>78675968</td>\n",
       "      <td>7.87e+07</td>\n",
       "      <td>1.941752e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LM_head</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>2634914201600</td>\n",
       "      <td>2.63e+12</td>\n",
       "      <td>6.503067e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>135060078493696</td>\n",
       "      <td>1.35e+14</td>\n",
       "      <td>3.333333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backward_pass_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>270120156987392</td>\n",
       "      <td>2.70e+14</td>\n",
       "      <td>6.666667e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall_TransformerLM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <td>405180235481088</td>\n",
       "      <td>4.05e+14</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           FLOPs  \\\n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                   78675968   \n",
       "                            MultiHeadAttentio         QKV_proj                      251658240000   \n",
       "                                                      Attention_scores              858993459200   \n",
       "                                                      Softmax                        33554432000   \n",
       "                                                      Attention_weighted            858993459200   \n",
       "                                                      O_proj                         83886080000   \n",
       "                                                      Overall_MultiHeadAttentio    2087085670400   \n",
       "                            RMSNorm_2                                                   78675968   \n",
       "                            FFN                                                     671612928000   \n",
       "                            Overall_Transformer_block                            132425085616128   \n",
       "RMSNorm_final                                                                           78675968   \n",
       "LM_head                                                                            2634914201600   \n",
       "forward_pass_TransformerLM                                                       135060078493696   \n",
       "backward_pass_TransformerLM                                                      270120156987392   \n",
       "Overall_TransformerLM                                                            405180235481088   \n",
       "\n",
       "                                                                                FLOPs_fmt  \\\n",
       "Level-1                     Level-2                   Level-3                               \n",
       "Transformer_block           RMSNorm_1                                            7.87e+07   \n",
       "                            MultiHeadAttentio         QKV_proj                   2.52e+11   \n",
       "                                                      Attention_scores           8.59e+11   \n",
       "                                                      Softmax                    3.36e+10   \n",
       "                                                      Attention_weighted         8.59e+11   \n",
       "                                                      O_proj                     8.39e+10   \n",
       "                                                      Overall_MultiHeadAttentio  2.09e+12   \n",
       "                            RMSNorm_2                                            7.87e+07   \n",
       "                            FFN                                                  6.72e+11   \n",
       "                            Overall_Transformer_block                            1.32e+14   \n",
       "RMSNorm_final                                                                    7.87e+07   \n",
       "LM_head                                                                          2.63e+12   \n",
       "forward_pass_TransformerLM                                                       1.35e+14   \n",
       "backward_pass_TransformerLM                                                      2.70e+14   \n",
       "Overall_TransformerLM                                                            4.05e+14   \n",
       "\n",
       "                                                                                 FLOPs_proportion  \n",
       "Level-1                     Level-2                   Level-3                                      \n",
       "Transformer_block           RMSNorm_1                                                1.941752e-07  \n",
       "                            MultiHeadAttentio         QKV_proj                       6.211020e-04  \n",
       "                                                      Attention_scores               2.120028e-03  \n",
       "                                                      Softmax                        8.281360e-05  \n",
       "                                                      Attention_weighted             2.120028e-03  \n",
       "                                                      O_proj                         2.070340e-04  \n",
       "                                                      Overall_MultiHeadAttentio      5.151006e-03  \n",
       "                            RMSNorm_2                                                1.941752e-07  \n",
       "                            FFN                                                      1.657566e-03  \n",
       "                            Overall_Transformer_block                                3.268301e-01  \n",
       "RMSNorm_final                                                                        1.941752e-07  \n",
       "LM_head                                                                              6.503067e-03  \n",
       "forward_pass_TransformerLM                                                           3.333333e-01  \n",
       "backward_pass_TransformerLM                                                          6.666667e-01  \n",
       "Overall_TransformerLM                                                                1.000000e+00  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_xl = flops_table_multiindex(\n",
    "    vocab_size=50257,\n",
    "    context_len=16384,\n",
    "    n_layers=48,\n",
    "    d_model=1600,\n",
    "    n_heads=25,\n",
    "    d_ff=6400,\n",
    ")\n",
    "\n",
    "gpt2_xl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
